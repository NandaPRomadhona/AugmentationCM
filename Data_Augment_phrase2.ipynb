{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guten Morgen!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "translator= Translator(to_lang=\"German\")\n",
    "translation = translator.translate(\"Good Morning!\")\n",
    "print (translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "from xml.dom import minidom\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, lang):\n",
    "    new_txt = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    for t in text:\n",
    "        try:\n",
    "            t = re.sub(r\"(\\d+)\", lambda x: num2words(int(x.group(0)), lang = lang), t)\n",
    "            t = re.sub(r'\\-', ' ', t)\n",
    "            t = regex.sub(' ', t)\n",
    "            t = re.sub(r'\\s+', ' ', t)\n",
    "            new_txt.append(t)\n",
    "        except:\n",
    "            print(t)\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_data = []\n",
    "# with utils.open('enwiki-20200701-pages-articles-multistream.json.gz', 'rb') as f:\n",
    "#     for i, line in enumerate(tqdm(f)):\n",
    "#         article = json.loads(line)\n",
    "#         for section_title, section_text in zip(article['section_titles'], article['section_texts']):\n",
    "#             txt = preprocessing(section_text.split('. ') , 'en')\n",
    "#             en_data += txt\n",
    "#         if len(en_data) > 2188070:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( en_data[0:2000000], open( \"en_data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_data_raw = en_data[2000000:4188070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_data = en_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del en_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"/home/nanda/XLM_modif/data/mix/M2/mix.train\"\n",
    "test = \"/home/nanda/XLM_modif/data/mix/M2/mix.test\"\n",
    "valid = \"/home/nanda/XLM_modif/data/mix/M2/mix.valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(train, \"r\")\n",
    "X_train = []\n",
    "for x in f:\n",
    "    X_train.append(x.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(test, \"r\")\n",
    "X_test = []\n",
    "for x in f:\n",
    "    X_test.append(x.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(valid, \"r\")\n",
    "X_valid = []\n",
    "for x in f:\n",
    "    X_valid.append(x.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_ms = r\"\"\"\n",
    "  NP: {<DET|ADJ|NOUN.*>+<DET|ADJ|NOUN.*>+}  \n",
    "  PP: {<ADP><NP>} \n",
    "      {<ADP><PROPN>}\n",
    "  VP: {<VERB.*><NP|PP|CLAUSE|ADP>+$} \n",
    "      {<VERB*><NOUN*>}\n",
    "      {<PART><VERB>}\n",
    "  CLAUSE: {<NP><VP>}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_en = r\"\"\"\n",
    "  NP: {<DET|ADJ|NOUN.*>+<DET|ADJ|NOUN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<ADP><NP>} \n",
    "      {<ADP><PROPN>}# Chunk prepositions followed by NP\n",
    "  VP: {<VERB.*><NP|PP|CLAUSE|ADP>+$} # Chunk verbs and their arguments\n",
    "      {<VERB*><NOUN*>}\n",
    "      {<PRT><VERB>}\n",
    "  CLAUSE: {<NP><VP>}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(dt, lgs):\n",
    "    if lgs == 'ms':\n",
    "        grammar = grammar_ms\n",
    "    else:\n",
    "        grammar= grammar_en    \n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(dt)\n",
    "    max_deep = result.height()\n",
    "    phrase = []\n",
    "    for s in result.subtrees(lambda result: result.height() == 2):\n",
    "        bb = []\n",
    "        for dd in s.leaves():\n",
    "            bb.append(dd[0])\n",
    "        phrase.append(bb)\n",
    "    if max_deep != 3:\n",
    "        for s in result.subtrees(lambda result: result.height() == 3):\n",
    "            bb = []\n",
    "            for dd in s.leaves():\n",
    "                bb.append(dd[0])\n",
    "            phrase.append(bb)\n",
    "    token = [d[0] for d in dt]\n",
    "    idx = find_idx_word(phrase, token)\n",
    "    return idx, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_ratio = [0.7,0.6,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang = {'en': ['ms', 'ms','ms','ms', 'ms','zh-tw', 'zh-cn'], 'ms':['en','en', 'en', 'en', 'en', 'zh-tw', 'zh-cn']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_word(phrases, token):\n",
    "    idx = []\n",
    "    i = 0\n",
    "    for ip, phrase in enumerate(phrases):\n",
    "        idx_i = []\n",
    "        for p in phrase:\n",
    "            try :\n",
    "                i = token.index(p, i+1)\n",
    "            except:\n",
    "                i = 0\n",
    "                i = token.index(p, i)\n",
    "            idx_i.append(i)\n",
    "            idx_i = [idx_i[0], idx_i[len(idx_i)-1]]\n",
    "        idx.append(idx_i)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sent_with(data, text, index):\n",
    "    del text[0]\n",
    "    del index[0]\n",
    "    \n",
    "    #flagging\n",
    "    for idx in index:\n",
    "        for i in range(idx[0], idx[1]+1):\n",
    "            data[i] = None\n",
    "            \n",
    "    # replace None with specific value\n",
    "    for i, idx in enumerate(index):\n",
    "        x = idx[0]\n",
    "        data[x] = text[i]\n",
    "        \n",
    "    #remove None value\n",
    "    clear_none = []\n",
    "    for x in data:\n",
    "        if x != None:\n",
    "            clear_none.append(x)\n",
    "            \n",
    "    new_text = []\n",
    "    for sublist in clear_none:\n",
    "        if type(sublist) is list:\n",
    "            for item in sublist:\n",
    "                new_text.append(item)\n",
    "        else:\n",
    "            new_text.append(sublist)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_en_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===phrase\n",
      "[['X', 'Men', 'membership'], ['to', 'work'], ['the', 'Muir', 'Island', 'research', 'center', 'Polaris', 'Havok'], ['long', 'time', 'lover'], ['a', 'former', 'X', 'Man'], ['control', 'magnetism'], ['at', 'the', 'Muir', 'Island', 'research', 'center', 'Polaris', 'Havok']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "for num_dt, dt in enumerate(tqdm(X_train)):\n",
    "    if random.random() < replace_ratio[0]:\n",
    "        token = word_tokenize(dt)\n",
    "        token = nltk.pos_tag(token, tagset='universal')\n",
    "        idx, txt = get_phrase(token, 'en')\n",
    "        dt = [d[0] for d in token]\n",
    "        translate_txt = [['']]\n",
    "        translate_idx = [[0,0]]\n",
    "        for j in range(round(replace_ratio[1]*len(txt))):\n",
    "            i = random.randint(0, len(txt)-1)\n",
    "            if idx[i] not in translate_idx:\n",
    "                lang = random.choice(tgt_lang['ms'])\n",
    "                translator= Translator(from_lang='ms',to_lang=lang)\n",
    "                txt_trans = translator.translate(' '.join(txt[i]))\n",
    "                translate_txt.append(txt_trans.split())\n",
    "                translate_idx.append(idx[i])\n",
    "        dt = replace_sent_with(dt, translate_txt, translate_idx)\n",
    "        new_en_data.append(' '.join(dt))\n",
    "    else:\n",
    "        if random.random() < replace_ratio[2]:\n",
    "            try:\n",
    "                dt = translator.translate(dt, dest=random.choice(tgt_lang['ms']), src='ms').text\n",
    "            except:\n",
    "                dt = dt\n",
    "        new_en_data.append(dt)\n",
    "    if (num_dt+1) % 100 == 0:\n",
    "        print('in')\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nanda/XLM_modif/data/mix/Mix2/mix.train', 'w') as f:\n",
    "    for item in new_en_data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanda_gpu",
   "language": "python",
   "name": "nanda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
