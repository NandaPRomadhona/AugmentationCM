{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "from xml.dom import minidom\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, lang):\n",
    "    new_txt = []\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    for t in text:\n",
    "        try:\n",
    "            t = re.sub(r\"(\\d+)\", lambda x: num2words(int(x.group(0)), lang = lang), t)\n",
    "            t = re.sub(r'\\-', ' ', t)\n",
    "            t = regex.sub(' ', t)\n",
    "            t = re.sub(r'\\s+', ' ', t)\n",
    "            new_txt.append(t)\n",
    "        except:\n",
    "            t = t\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_data = []\n",
    "with utils.open('mswiki-20201001-pages-articles-multistream.json.gz', 'rb') as f:\n",
    "    for line in f:\n",
    "        article = json.loads(line)\n",
    "        for section_title, section_text in zip(article['section_titles'], article['section_texts']):\n",
    "            txt = preprocessing(section_text.split('. ') , 'id')\n",
    "            ms_data+= txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( ms_data, open( \"ms_data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_ms = r\"\"\"\n",
    "  NP: {<DET|ADJ|NOUN.*>+<DET|ADJ|NOUN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<ADP><NP>} \n",
    "      {<ADP><PROPN>}# Chunk prepositions followed by NP\n",
    "  VP: {<VERB.*><NP|PP|CLAUSE|ADP>+$} # Chunk verbs and their arguments\n",
    "      {<VERB*><NOUN*>}\n",
    "      {<PART><VERB>}\n",
    "  CLAUSE: {<NP><VP>}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_en = r\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(dt, lgs):\n",
    "    if lgs == 'ms':\n",
    "        grammar = grammar_ms\n",
    "    else:\n",
    "        grammar= grammar_en    \n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(dt)\n",
    "    max_deep = result.height()\n",
    "    phrase = []\n",
    "    for s in result.subtrees(lambda result: result.height() == 2):\n",
    "        bb = []\n",
    "        for dd in s.leaves():\n",
    "            bb.append(dd[0])\n",
    "        phrase.append(bb)\n",
    "    if max_deep != 3:\n",
    "        for s in result.subtrees(lambda result: result.height() == 3):\n",
    "            bb = []\n",
    "            for dd in s.leaves():\n",
    "                bb.append(dd[0])\n",
    "            phrase.append(bb)\n",
    "    token = [d[0] for d in dt]\n",
    "    idx = find_idx_word(phrase, token)\n",
    "    return idx, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_ratio = [0.7,0.6,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang = {'en': ['ms', 'ms','ms','ms', 'ms','zh-tw', 'zh-cn'], 'ms':['en','en', 'en', 'en', 'en', 'zh-tw', 'zh-cn']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_word(phrases, token):\n",
    "    idx = []\n",
    "    i = 0\n",
    "    for ip, phrase in enumerate(phrases):\n",
    "        idx_i = []\n",
    "        for p in phrase:\n",
    "            try :\n",
    "                i = token.index(p, i+1)\n",
    "            except:\n",
    "                i = 0\n",
    "                i = token.index(p, i)\n",
    "            idx_i.append(i)\n",
    "            idx_i = [idx_i[0], idx_i[len(idx_i)-1]]\n",
    "        idx.append(idx_i)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sent_with(data, text, index):\n",
    "    del text[0]\n",
    "    del index[0]\n",
    "    \n",
    "    #flagging\n",
    "    for idx in index:\n",
    "        for i in range(idx[0], idx[1]+1):\n",
    "            data[i] = None\n",
    "            \n",
    "    # replace None with specific value\n",
    "    for i, idx in enumerate(index):\n",
    "        x = idx[0]\n",
    "        data[x] = text[i]\n",
    "        \n",
    "    #remove None value\n",
    "    clear_none = []\n",
    "    for x in data:\n",
    "        if x != None:\n",
    "            clear_none.append(x)\n",
    "            \n",
    "    new_text = []\n",
    "    for sublist in clear_none:\n",
    "        if type(sublist) is list:\n",
    "            for item in sublist:\n",
    "                new_text.append(item)\n",
    "        else:\n",
    "            new_text.append(sublist)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ms_data = []\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import malaya\n",
    "model = malaya.pos.transformer(model = 'albert')\n",
    "# tokens = model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ms_data = pickle.load( open( \"new_ms_phrase_augment.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2129567 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iklim di Laut Carribean dipengaruhi oleh Arus Humbolt dari utara\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2129567 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iklim', '在海上', 'Carribean', 'dipengaruhi', 'oleh', 'Arus', 'Humbolt', 'dari', 'utara']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for num_dt, dt in enumerate(tqdm(ms_data[len(new_ms_data):])):\n",
    "    print(dt)\n",
    "    if random.random() < replace_ratio[0]:\n",
    "        try:\n",
    "            token = model.predict(dt)\n",
    "        except:\n",
    "            continue\n",
    "        idx, txt = get_phrase(token, 'ms')\n",
    "        dt = [d[0] for d in token]\n",
    "        translate_txt = [['']]\n",
    "        translate_idx = [[0,0]]\n",
    "        for j in range(round(replace_ratio[1]*len(txt))):\n",
    "            i = random.randint(0, len(txt)-1)\n",
    "            if idx[i] not in translate_idx:\n",
    "                lang = random.choice(tgt_lang['ms'])\n",
    "                txt_trans = translator.translate((' '.join(txt[i])), dest=lang, src='ms')\n",
    "                translate_txt.append(txt_trans.text.split())\n",
    "                translate_idx.append(idx[i])\n",
    "        dt = replace_sent_with(dt, translate_txt, translate_idx)\n",
    "        new_ms_data.append(' '.join(dt))\n",
    "    else:\n",
    "        if random.random() < replace_ratio[2]:\n",
    "            try:\n",
    "                dt = translator.translate(dt, dest=random.choice(tgt_lang['ms']), src='ms').text\n",
    "            except:\n",
    "                dt = dt\n",
    "        new_ms_data.append(dt)\n",
    "    print(dt)\n",
    "    break\n",
    "    if (num_dt+1) % 100 == 0:\n",
    "        print('sleep')\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Along the coast terdapat teluk dan tanjung yang berpontensi dimajukan sebagai tourist areas'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['Along', 'the', 'coast', 'terdapat', 'teluk', 'dan', 'tanjung', 'yang', 'berpontensi', 'dimajukan', 'sebagai', 'tourist', 'areas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( new_ms_data, open( \"new_ms_phrase_augment.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load( open( \"new_ms_phrase_augment.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58504"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanda_gpu",
   "language": "python",
   "name": "nanda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
